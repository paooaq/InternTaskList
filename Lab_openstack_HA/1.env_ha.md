# Mô hình cài đặt

###### Mô hình cài đặt lab OpenStack Victoria trên Ubuntu 20.04 64-bit dựng trên openstack

![env_ha](D:\OneDrive\FPT\tasklist\image\env_ha.png)

IP PLANNING

![ip_planning](D:\OneDrive\FPT\tasklist\image\ip_planning.PNG)

**VIP: 192.168.30.212**

# Cài đặt môi trường

**Đặt địa chỉ IP như bảng IP PLANNING. Apply để áp dụng thay đổi:**

`netplan apply`

**Cấu hình file hosts**

`192.168.30.212	controller`
`192.168.30.10	controller1`
`192.168.30.11	controller2`
`192.168.30.12	controller3`
`192.168.30.30	compute1`
`192.168.30.31	compute2`

## Cài đặt percona XtraDB trên 3 node controller

1. #### Cấu hình kho lưu trữ Percona

   - ```
     sudo apt-get update
     sudo apt-get install -y wget gnupg2 curl lsb-release 
     ```

   - Fetch the repository package: 

     ```
     wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb
     ```

   - Install the downloaded repository package with `dpkg`:

     ```
     sudo dpkg -i percona-release_latest.generic_all.deb
     ```

   - Check the repository setup for the Percona original release list in the `/etc/apt/sources.list.d/percona-original-release.list` file

   - Refresh the local cache to update the package information:

     ```
     sudo apt-get update
     ```

   - enable-only PXC-80 release

     ```
     sudo percona-release enable-only pxc-80 experimental
     sudo percona-release enable tools release
     ```

   - install percona extradb cluster :

     ```
     sudo apt update
     sudo apt-get install percona-xtradb-cluster
     ```

2. #### Cấu hình động bộ giữa các node

   - Dừng dịch vụ mysql trước khi cấu hình

     ```
     systemctl stop mysql
     ```

   - Cấu hình mỗi server (/etc/mysql/mysql.conf.d/mysqld.cnf)
   
     Trên controller1:
   
     ```
     [client]
     socket=/var/run/mysqld/mysqld.sock
     [mysqld]
     server-id=1
     datadir=/var/lib/mysql
     socket=/var/run/mysqld/mysqld.sock
     log-error=/var/log/mysql/error.log
     pid-file=/var/run/mysqld/mysqld.pid
     binlog_expire_logs_seconds=604800
     wsrep_provider=/usr/lib/galera4/libgalera_smm.so
     wsrep_cluster_address=gcomm://192.168.30.10,192.168.30.11,192.168.30.12
     binlog_format=ROW
     wsrep_slave_threads=8
     wsrep_log_conflicts
     innodb_autoinc_lock_mode=2
     wsrep_node_address=192.168.30.10
     wsrep_cluster_name=controller
     wsrep_node_name=controller1
     pxc_strict_mode=ENFORCING
     wsrep_sst_method=xtrabackup-v2
     ```
   
     Trên controller2:
   
     ```
     [client]
     socket=/var/run/mysqld/mysqld.sock
     [mysqld]
     server-id=1
     datadir=/var/lib/mysql
     socket=/var/run/mysqld/mysqld.sock
     log-error=/var/log/mysql/error.log
     pid-file=/var/run/mysqld/mysqld.pid
     binlog_expire_logs_seconds=604800
     wsrep_provider=/usr/lib/galera4/libgalera_smm.so
     wsrep_cluster_address=gcomm://192.168.30.10,192.168.30.11,192.168.30.12
     binlog_format=ROW
     wsrep_slave_threads=8
     wsrep_log_conflicts
     innodb_autoinc_lock_mode=2
     wsrep_node_address=192.168.30.11
     wsrep_cluster_name=controller
     wsrep_node_name=controller-2
     pxc_strict_mode=ENFORCING
     wsrep_sst_method=xtrabackup-v2
     ```
   
     Trên controller3:
   
     ```
     [client]
     socket=/var/run/mysqld/mysqld.sock
     [mysqld]
     server-id=1
     datadir=/var/lib/mysql
     socket=/var/run/mysqld/mysqld.sock
     log-error=/var/log/mysql/error.log
     pid-file=/var/run/mysqld/mysqld.pid
     binlog_expire_logs_seconds=604800
     wsrep_provider=/usr/lib/galera4/libgalera_smm.so
     wsrep_cluster_address=gcomm://192.168.30.10,192.168.30.11,192.168.30.12
     binlog_format=ROW
     wsrep_slave_threads=8
     wsrep_log_conflicts
     innodb_autoinc_lock_mode=2
     wsrep_node_address=192.168.30.12
     wsrep_cluster_name=controller
     wsrep_node_name=controller3
     pxc_strict_mode=ENFORCING
     wsrep_sst_method=xtrabackup-v2
     ```

Chọn controller1 làm node đầu tiên để khởi tạo cụm:

```
systemctl start mysql@bootstrap.service
```

Bật mysql service để controller2,controller3 tham gia vào cụm:

```
systemctl start mysql
```

Vào mysql shell để tăng max_connection trên cả 3 node:

```
SET GLOBAL max_connections=4096;
```

Kiểm tra trạng thái của cluster:

```
show status like 'wsrep%';
```

## Cài đặt keepalived trên 3 node controller

`apt install keepalived -y`

Cấu hình keepalived trong file `/etc/keepalived/keepalived.conf`

Trên controller 1:

```
vrrp_sync_group VG_1 {
    group {
        WAN_LAN_1
    }
}

vrrp_instance WAN_LAN_1 {
    state BACKUP
    interface eth1
    virtual_router_id 10
    dont_track_primary
    priority 100
    preempt_delay 30
    garp_master_delay 1
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass dd9Mke2L2DNX
    }
    track_interface {
        eth1
    }
    virtual_ipaddress {
        192.168.30.212/24 dev eth1
    }
}
```

Trên controller 2:

```
vrrp_sync_group VG_1 {
    group {
        WAN_LAN_1
    }
}

vrrp_instance WAN_LAN_1 {
    state BACKUP
    interface eth1
    virtual_router_id 10
    dont_track_primary
    priority 99
    preempt_delay 30
    garp_master_delay 1
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass dd9Mke2L2DNX
    }
    track_interface {
        eth1
    }
    virtual_ipaddress {
        192.168.30.212/24 dev eth1
    }
}
```

Trên controller 3:

```
vrrp_sync_group VG_1 {
    group {
        WAN_LAN_1
    }
}

vrrp_instance WAN_LAN_1 {
    state BACKUP
    interface eth1
    virtual_router_id 10
    dont_track_primary
    priority 98
    preempt_delay 30
    garp_master_delay 1
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass dd9Mke2L2DNX
    }
    track_interface {
        eth1
    }
    virtual_ipaddress {
        192.168.30.212/24 dev eth1
    }
}
```

restart service:

```
service keepalived restart
```

## Cấu hình cần thiết

Cấu hình controller nonlocal bind và forward packet trong file `/etc/sysctl.conf`

```
net.ipv4.ip_nonlocal_bind = 1
net.ipv4.ip_forward = 1
```

Load lại sysctl

```
sysctl -p
```

## Cài đặt NTP

Trên tất cả các node cài đặt NTP

```
apt-get install -y chrony
```

#### Controller Node

Cấu hình file `/etc/chrony/chrony.conf` như sau:

```
server 1.vn.pool.ntp.org iburst
server 0.asia.pool.ntp.org iburst 
server 3.asia.pool.ntp.org iburst
allow 192.168.30.0/24
```

Restart dịch vụ NTP

```
service chrony restart
```

Kiểm tra hoạt động của NTP

```
root@controller1:~# chronyc sources
210 Number of sources = 3
MS Name/IP address         Stratum Poll Reach LastRx Last sample               
===============================================================================
^* time.cloudflare.com           3   6   377    46  -1437us[-1486us] +/-   59ms
^- time4.isu.net.sa              2  10   377   684    +18ms[  +18ms] +/-  249ms
^+ time.iqnet.com                2  10   377   586  +1816us[+1706us] +/-  173ms
```

##### Compute Node

Trong file `/etc/chrony/chrony.conf`

```
server controller iburst
```

Restart dịch vụ NTP

```
service chrony restart
```

Kiểm tra hoạt động của NTP

```
root@compute1:~# chronyc sources
210 Number of sources = 1
MS Name/IP address         Stratum Poll Reach LastRx Last sample               
===============================================================================
^* controller                    4   7   377    33    -49us[ -138us] +/-   59ms
```

## Cài đặt Openstack Victoria

Trên tất cả các node cài đặt Openstack Victoria

```
add-apt-repository cloud-archive:victoria
```

Cài đặt gói openstack client

```
apt install python3-openstackclient
```

## Memcached

Cơ chế xác thực của KeyStone cho các dịch vụ sử dụng memcached để cache các token. Memcached thường được cài đặt trên controller node.

```
apt install memcached python3-memcache
```

Cấu hình file `/etc/memcached.conf` và cấu hình dịch vụ sử dụng VIP management của controller node để cho phép các node khác truy cập thông qua management node

```
-d
logfile /var/log/memcached.log
-m 64
-p 11211
-u memcache
-l 192.168.30.212
```

Khởi động lại memcached

```
service memcached restart
```

## RabbitMQ

Cài đặt rabbitmq trên 3 node controller

```
apt install -y rabbitmq-server
```

Stop rabbitmq trên node controller2, node controller3

```
service rabbitmq-server stop
```

Lấy erlang cookie từ node controller1 chuyển sang node controller2 và node controller3

```
root@controller1:~# cat /var/lib/rabbitmq/.erlang.cookie
root@controller2:~# sudo sh -c "echo 'COOKIE_FROM_CONTROLLER1' > /var/lib/rabbitmq/.erlang.cookie"
root@controller3:~# sudo sh -c "echo 'COOKIE_FROM_CONTROLLER1' > /var/lib/rabbitmq/.erlang.cookie"
```

Start rabbitmq trên node controller2 và node controller3

```
service rabbitmq-server start
```

Join node controller2,controller3 vào cluster

```
rabbitmqctl stop_app
rabbitmqctl join_cluster rabbit@controller1
rabbitmqctl start_app
```

Check status

```
rabbitmqctl cluster_status
```

Config replicate toàn bộ queue trên tất cả các node (trừ các queue mặc định)

```
rabbitmqctl set_policy ha-all '^(?!amq\.).*' '{"ha-mode": "all"}'
```

Enable plugin rabbitmq management trên cả 3 node controller.

```
rabbitmq-plugins enable rabbitmq_management
```

Tạo user trên rabbitmq và set permission (1 trong 3 node controller)

```
rabbitmqctl add_user openstack RABBIT_PASS
rabbitmqctl set_user_tags openstack administrator
rabbitmqctl set_permissions openstack ".*" ".*" ".*"
```

Cấu hình rabbitmq bind IP management trong file `/etc/rabbitmq/rabbitmq.config`

Trên controller1:

```
[
    {rabbit,
        [
            {tcp_listeners, [{"192.168.30.10", 5672}]},
            {vm_memory_high_watermark, 0.6}
        ]
    },
    {rabbitmq_management,
        [
            {listener, [{port, 15672},
                        {ip, "192.168.30.10"}]}
        ]
    }
].
```

Trên controller2:

```
[
    {rabbit,
        [
            {tcp_listeners, [{"192.168.30.11", 5672}]},
            {vm_memory_high_watermark, 0.6}
        ]
    },
    {rabbitmq_management,
        [
            {listener, [{port, 15672},
                        {ip, "192.168.30.11"}]}
        ]
    }
].
```

Trên controller3:

```
[
    {rabbit,
        [
            {tcp_listeners, [{"192.168.30.12", 5672}]},
            {vm_memory_high_watermark, 0.6}
        ]
    },
    {rabbitmq_management,
        [
            {listener, [{port, 15672},
                        {ip, "192.168.30.12"}]}
        ]
    }
].
```

Restart service rabbitmq

```
service rabbitmq-server restart
```

## Cài đặt Haproxy

Cài haproxy trên 3 node controller

```
apt install -y haproxy
```

Cho phép haproxy chạy khi server khởi động lại

```
echo ENABLED=1 >> /etc/default/haproxy
```

Cấu hình file `/etc/haproxy/haproxy.cfg` trên cả 3 node controller :

```
global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

	# Default SSL material locations
	ca-base /etc/ssl/certs
	crt-base /etc/ssl/private

	# See: https://ssl-config.mozilla.org/#server=haproxy&server-version=2.0.3&config=intermediate
        ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
        ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256
        ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets

defaults
	log	global
	mode	http
	option	httplog
	option	dontlognull
        timeout connect 5000
        timeout client  50000
        timeout server  50000
	errorfile 400 /etc/haproxy/errors/400.http
	errorfile 403 /etc/haproxy/errors/403.http
	errorfile 408 /etc/haproxy/errors/408.http
	errorfile 500 /etc/haproxy/errors/500.http
	errorfile 502 /etc/haproxy/errors/502.http
	errorfile 503 /etc/haproxy/errors/503.http
	errorfile 504 /etc/haproxy/errors/504.http

frontend stats
    bind *:1936
    mode http
    stats enable
    stats hide-version
    stats realm Haproxy\ Statistics
    stats uri /
    stats auth admin:fptcloud123
    stats refresh 30s


frontend keystone_internal
  bind 192.168.30.212:5000
  mode  http
  use_backend keystone_internal

backend keystone_internal
  balance  source
  mode  http
  option httpchk GET /
  http-check expect rstatus (2|3|4)[0-9][0-9]
  server  controller1  192.168.30.10:5000  check inter 2000 rise 2 fall 5
  server  controller2  192.168.30.11:5000  check inter 2000 rise 2 fall 5
  server  controller3  192.168.30.12:5000  check inter 2000 rise 2 fall 5

frontend glance_api_internal
  bind 192.168.30.212:9292
  mode  http
  use_backend glance_api_internal

backend glance_api_internal
  balance  source
  mode  http
  option httpchk GET /
  http-check expect rstatus (2|3|4)[0-9][0-9]
  server  controller1  192.168.30.10:9292  check inter 2000 rise 2 fall 5
  server  controller2  192.168.30.11:9292  check inter 2000 rise 2 fall 5
  server  controller3  192.168.30.12:9292  check inter 2000 rise 2 fall 5

frontend nova_api_internal
  bind 192.168.30.212:8774
  mode  http
  use_backend nova_api_internal

backend nova_api_internal
  balance  source
  mode  http
  option httpchk GET /
  http-check expect rstatus (2|3|4)[0-9][0-9]
  server  controller1  192.168.30.10:8774  check inter 2000 rise 2 fall 5
  server  controller2  192.168.30.11:8774  check inter 2000 rise 2 fall 5
  server  controller3  192.168.30.12:8774  check inter 2000 rise 2 fall 5

frontend nova-placement-api_internal
  bind 192.168.30.212:8778
  mode  http
  use_backend nova-placement-api_internal

backend nova-placement-api_internal
  balance  source
  mode  http
  option httpchk GET /
  http-check expect rstatus (2|3|4)[0-9][0-9]
  server  controller1  192.168.30.10:8778  check inter 2000 rise 2 fall 5
  server  controller2  192.168.30.11:8778  check inter 2000 rise 2 fall 5
  server  controller3  192.168.30.12:8778  check inter 2000 rise 2 fall 5

frontend nova_metadata_api_internal
  bind 192.168.30.212:8775
  mode  http
  use_backend nova_metadata_api_internal

backend nova_metadata_api_internal
  balance  source
  mode  http
  option httpchk GET /
  http-check expect rstatus (2|3|4)[0-9][0-9]
  server  controller1  192.168.30.10:8775  check inter 2000 rise 2 fall 5
  server  controller2  192.168.30.11:8775  check inter 2000 rise 2 fall 5
  server  controller3  192.168.30.12:8775  check inter 2000 rise 2 fall 5

frontend nova_novncproxy_api_internal
  bind 192.168.30.212:6080
  mode  http
  use_backend nova_novncproxy_api_internal

backend nova_novncproxy_api_internal
  balance  source
  mode  http
  option httpchk GET /
  http-check expect rstatus (2|3|4)[0-9][0-9]
  server  controller1  192.168.30.10:6080  check inter 2000 rise 2 fall 5
  server  controller2  192.168.30.11:6080  check inter 2000 rise 2 fall 5
  server  controller3  192.168.30.12:6080  check inter 2000 rise 2 fall 5

frontend neutron_api_internal
  bind 192.168.30.212:9696
  mode  http
  use_backend neutron_api_internal

backend neutron_api_internal
  balance  source
  mode  http
  option httpchk GET /
  http-check expect rstatus (2|3|4)[0-9][0-9]
  server  controller1  192.168.30.10:9696  check inter 2000 rise 2 fall 5
  server  controller2  192.168.30.11:9696  check inter 2000 rise 2 fall 5
  server  controller3  192.168.30.12:9696  check inter 2000 rise 2 fall 5

frontend Horizon
  mode  http
  use_backend Horizon

backend Horizon
  balance  source
  mode  http
  option httpchk GET /
  http-check expect rstatus (2|3|4)[0-9][0-9]
  server  controller1  192.168.30.10:80  check inter 2000 rise 2 fall 5
  server  controller2  192.168.30.11:80  check inter 2000 rise 2 fall 5
  server  controller3  192.168.30.12:80  check inter 2000 rise 2 fall 5

frontend Horizon_SSL
  bind 192.168.30.212:443
  mode  http
  use_backend Horizon_SSL

backend Horizon_SSL
  balance  source
  mode  http
  option httpchk GET /
  http-check expect rstatus (2|3|4)[0-9][0-9]
  server  controller1  192.168.30.10:80  check inter 2000 rise 2 fall 5
  server  controller2  192.168.30.11:80  check inter 2000 rise 2 fall 5
  server  controller3  192.168.30.12:80  check inter 2000 rise 2 fall 5
```

Restart service haproxy

```
service haproxy restart
```

